{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is also in standalone python scripts. Based on a Google Plus article on getting data from webscaping led to interesting playing with selenium and beautifulsoup. There's another set of code for volcanoes that I'd like to bump up to do some playing with scikit-learn and maybe TensorFlow.\n",
    "Here, create two functions, get_all_earthquakes and getEarthquakeDetails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "def get_all_earthquakes(html_soup):\n",
    "    earthquakes = html_soup.findAll(\"li\",attrs={'class':'eqitem'})\n",
    "    earthquakedata = []\n",
    "    #sys.stderr.write(repr(html_soup)+'\\n')\n",
    "    for earthquake in earthquakes:\n",
    "        title = earthquake.find(attrs={'class':'title'})\n",
    "        data = earthquake.findAll('span')\n",
    "        row_cells = earthquake.findAll(\"span\")\n",
    "        earthquake_entry = {\n",
    "            \"name\":title.text,\n",
    "            \"usgsref\":title['href'],\n",
    "            \"strength\":row_cells[0].text.strip(),\n",
    "            \"datetime\":row_cells[1].text.strip(),\n",
    "            \"depthKM\":row_cells[2].text.strip().split()[0]\n",
    "        }\n",
    "        earthquakedata.append(earthquake_entry)\n",
    "    return earthquakedata\n",
    "\n",
    "def getEarthquakeDetails(soup):\n",
    "    details = {}\n",
    "    # the usgs page has a *lot* more data than what I'm grabbing now.\n",
    "    evheader = soup.find('header',attrs={'class':'event-header'})\n",
    "    location = evheader.find('span',attrs={'class':'event-coordinates'})\n",
    "    details['location'] = location.text\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to the 'main'. Makes use of selenium package which connects as a Chrome browser to usgs earthquake data.\n",
    "Then pass parsed html to get_all_earthquakes, build dataframe out of it,\n",
    "get more details and build a more detailed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                  datetime depthKM                                  location  \\\n",
      "0  2001-12-28 22:09:28 UTC   160.6    \\n        8.357°S     \\n    74.220°W     \n",
      "1  2001-12-27 10:54:51 UTC   153.2  \\n        14.647°S     \\n    167.262°E     \n",
      "2  2001-12-23 22:52:54 UTC    16.0   \\n        9.613°S     \\n    159.530°E     \n",
      "3  2001-12-22 00:40:04 UTC    74.2  \\n        10.910°S     \\n    165.863°E     \n",
      "4  2001-12-18 04:02:58 UTC    14.0  \\n        23.954°N     \\n    122.734°E     \n",
      "\n",
      "                 name strength  \\\n",
      "0        central Peru      6.0   \n",
      "1             Vanuatu      6.2   \n",
      "2     Solomon Islands      6.8   \n",
      "3  Santa Cruz Islands      6.0   \n",
      "4       Taiwan region      6.8   \n",
      "\n",
      "                                             usgsref  \n",
      "0  https://earthquake.usgs.gov/earthquakes/eventp...  \n",
      "1  https://earthquake.usgs.gov/earthquakes/eventp...  \n",
      "2  https://earthquake.usgs.gov/earthquakes/eventp...  \n",
      "3  https://earthquake.usgs.gov/earthquakes/eventp...  \n",
      "4  https://earthquake.usgs.gov/earthquakes/eventp...  \n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.set_headless(True)\n",
    "browser = webdriver.Chrome(chrome_options=options)\n",
    "browser.get('https://earthquake.usgs.gov/earthquakes/browse/m6-world.php?year=2001')\n",
    "html = browser.page_source\n",
    "html_soup = BeautifulSoup(html, 'html.parser')\n",
    "earthquake_list = get_all_earthquakes(html_soup)\n",
    "earthquakedf = pd.DataFrame(earthquake_list)\n",
    "# This part is really slow. Probably more sense to use the fdsnws\n",
    "# catalog with a dedicated api (https://earthquake.usgs.gov/fdsnws/).\n",
    "for earthquake in earthquake_list:\n",
    "    browser.get(earthquake['usgsref'])\n",
    "    detailSoup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    earthquake.update(getEarthquakeDetails(detailSoup))\n",
    "earthquakedf = pd.DataFrame(earthquake_list)\n",
    "sys.stderr.write(repr(earthquakedf.head(5)) +'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
